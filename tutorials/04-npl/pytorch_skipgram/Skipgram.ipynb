{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129b79e4",
   "metadata": {},
   "source": [
    "## Skipgram in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ae9b9",
   "metadata": {},
   "source": [
    "### Good reads before going through the codes\n",
    "\n",
    "-[Tutorial build your own embedding](https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296)\n",
    "\n",
    "-[Code](https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/02.Skip-gram-Negative-Sampling.ipynb)\n",
    "\n",
    "-[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf66f5",
   "metadata": {},
   "source": [
    "## 1. Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3132a",
   "metadata": {},
   "source": [
    "![skipgram](https://cdn-images-1.medium.com/max/800/1*SR6l59udY05_bUICAjb6-w.png)\n",
    "\n",
    "\n",
    "### (1) Softmax Objective\n",
    "Skip-gram’s objective is to predict the contexts, given a target word: $V_t \\rightarrow V_c$ \n",
    "\n",
    "The contexts are immediate neighbours of the target and are retrieved using a window of an arbitrary size _n_ \n",
    "    \n",
    "Capturing _n_ words to the left of the target and _n_ words to its right.\n",
    "\n",
    "In a two-gram example:\n",
    "\n",
    "$$\\underbrace{\\textrm{The quick}}_{\\textrm{left } n}\\underbrace{\\textrm{ brown }}_{target} \\underbrace{\\textrm{for jumps}}_{\\textrm{right } n}$$\n",
    "\n",
    "<img src=\"https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/images/01.skipgram-prepare-data.png\">\n",
    "\n",
    "The original Skip-gram's objective is to maximise $P(V_c|V_t)$: The probability of $V_c$ being predicted as $V_t$’s context for all training pairs.\n",
    "\n",
    "![objective](https://miro.medium.com/max/700/1*aBBoTq_b_zQBwR2gi9mMkA.png)\n",
    "\n",
    "To calculate $P(V_c|V_t)$ we need a way to quantify the __closeness__ of the target-word and the context-word. \n",
    "\n",
    "In Skip-gram, this closeness is computed using the __dot product between the input-embedding of the target and the output-embedding of the context__.\n",
    "\n",
    "Now, if we define $u_{t,c}$ to be the measure of words' closeness between the target word and context word, $E$ to be the embedding matrix holding input-embeddings and $O$ to be the output-embedding matrix we get:\n",
    "\n",
    "![dotproduct](https://miro.medium.com/max/372/1*z6pc-3WV7CGiJQq2Xca7bA.png)\n",
    "\n",
    "\n",
    "which we can use to compute P(Vc|Vt) using the softmax function:\n",
    "\n",
    "![softmax](https://miro.medium.com/max/2400/1*tWAXNe83b5Le9mKx0oeIpQ.png)\n",
    "\n",
    "\n",
    "\n",
    "### (2) Architecture\n",
    "In terms of the architecture, Skip-gram is a simple neural network with only one hidden layer. The input to the network is a one-hot encoded vector representation of a target-word — all of its dimensions are set to zero, apart from the dimension corresponding to the target-word. The output is the probability distribution over all words in the vocabulary which defines the likelihood of a word being selected as the input word’s context:\n",
    "\n",
    "![archit1](https://miro.medium.com/max/2400/1*4Viy_LvP6jLIWSvB9-Fk-Q.png)\n",
    "\n",
    "![archit2](https://miro.medium.com/max/456/1*q-BAjhE79g7_od8XuXC4Xg.png)\n",
    "\n",
    "\n",
    "### (3) Negative Sampling\n",
    "\n",
    "But there is an issue with the original softmax objective of Skip-gram — it is highly computationally expensive, as it requires scanning through the output-embeddings of all words in the vocabulary in order to calculate the sum from the denominator. And typically such vocabularies contain hundreds of thousands of words. Because of this inefficiency most implementations use an alternative, negative-sampling objective, which rephrases the problem as a set of independent binary classification tasks.\n",
    "\n",
    "Instead of defining the complete probability distribution over words, the model learns to differentiate between the correct training pairs retrieved from the corpus and the incorrect, randomly generated pairs. For each correct pair the model draws m negative ones — with m being a hyperparameter. All negative samples have the same Vt as the original training pair, but their Vc is drawn from an arbitrary noise distribution. Building on the previous example, for the training pair (fox, jump) the incorrect ones could be (fox, man) or (fox, truck). The new objective of the model is to maximise the probability of the correct samples coming from the corpus and minimise the corpus probability for the negative samples, such as (fox, truck).\n",
    "\n",
    "Let’s set D to be the set of all correct pairs and D’ to denote a set of all negatively sampled |D| × m pairs. We will also define P(C = 1|Vt, Vc) to be the probability of (Vt , Vc) being a correct pair, originating from the corpus. Given this setting, the negative-sampling objective is defined as maximising:\n",
    "\n",
    "![negsamequ](https://miro.medium.com/max/2400/1*Uns2130EOoQY0AM9Lwf9fg.png)\n",
    "\n",
    "\n",
    "Since this time for each sample we are making a binary decision we define P(C = 1|Vt, Vc) using the sigmoid function:\n",
    "![nesamequ2](https://miro.medium.com/max/485/1*V5g7NuFg_cyYranubVRYcA.png)\n",
    "\n",
    "where, as before, uc = Et · Oc. Now, if we plug this into the previous negative-sampling equation and simplify a little we get the following objective:\n",
    "\n",
    "![negsamequ3](https://miro.medium.com/max/584/1*YZhOEMKyZGDEoAi8OOUDBA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343fbc9b",
   "metadata": {},
   "source": [
    "## 2. Import and Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "384e2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import pdb\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a9c1c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getBatch function return data in batches\n",
    "\n",
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        sindex = eindex\n",
    "        eindex += batch_size\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873a5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_sequence change sentance to list of index eg [5 124 421 0 3764 124 1 345]\n",
    "# prepare_word change words to index eg 4523\n",
    "# index type is longTensor (64-bit integer)\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.Tensor(idxs).type(torch.LongTensor)\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return torch.Tensor([word2index[word]]).type(torch.LongTensor) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8fda9b",
   "metadata": {},
   "source": [
    "## 3. Data loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ad4e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\tanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download corpus\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3954261",
   "metadata": {},
   "source": [
    "##### Gutenberg Corpus\n",
    "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/. We begin by getting the Python interpreter to load the NLTK package, then ask to see `nltk.corpus.gutenberg.fileids()`, the file identifiers in this corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef6c07c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9711b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']'] 8\n"
     ]
    }
   ],
   "source": [
    "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:500] # sampling sentences for test\n",
    "# lowercase\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]\n",
    "\n",
    "print(len(corpus))\n",
    "print(corpus[0], len(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b0694c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude sparse words\n",
    "# word count more than 3 are remove\n",
    "\n",
    "MIN_COUNT = 3\n",
    "word_count = Counter(flatten(corpus))\n",
    "exclude = []\n",
    "for w, c in word_count.items():\n",
    "    if c < MIN_COUNT:\n",
    "        exclude.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c745d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129\n"
     ]
    }
   ],
   "source": [
    "print(len(exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb1d6f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['english', 'sometimes', 'london', 'did', 'chapter', 'entry', 'find', 't', 'chace', 'stern', 'water', 'she', 'cape', 'who', 'another', 'beast', 'less', 'there', 'red', 'and', 'dim', '?--', 'sing', 'passage', 'again', 'great', 'altogether', 'dictionary', 'teeth', 'winds', 'bright', 'compare', 'than', 'paying', 'mind', 'place', 'without', 'form', 'me', 'enter', 'he', 'black', 'myself', 'though', 'that', 'go', 'against', 'when', 'quantity', 'd', 'parts', 'last', 'till', 'could', 'under', 'name', 'jaw', 'royal', 'here', 'may', 'said', 'each', 'same', 'especially', '!\"', 'young', 'while', 'ibid', 'three', 'globe', 'summer', 'tail', 'whales', 'voyages', 'much', 'looking', 'eyes', 'sleeps', 'give', 'run', 'should', 'spermacetti', ',\"', 'too', 'would', 'open', 'island', 'feet', 'sperma', 'think', 'blows', 'monsters', 'sign', 'ago', 'whale', 'poor', 'tempestuous', 'inn', 'immense', 'blue', 'from', 'city', 'entering', '.\"', 'hand', 'if', 'ceti', 'history', 'within', 'making', 'northern', 'part', 'their', 'such', 'among', 'not', 'then', 'gone', 'board', 'thomas', 'account', 'passenger', 'found', 'voyage', 'south', 'we', 'way', 'lead', 'ha', 'by', 'monster', 'towards', 'am', 'her', 'bag', 'ocean', 'particular', 'side', 'known', 'only', 'right', 'nuee', '--', 'our', 'even', 'thinks', 'arched', 'stop', 'grow', 'frost', 'vast', 'take', 'something', 'jaws', 'came', '--\"', 'on', 'sail', 'whom', 'near', 'things', 'god', 'your', 'nantucket', 'swallow', 'day', 'goes', 'old', 'tears', 'ice', 'hearts', 'captain', 'killed', '?', 'upon', 'huge', 'light', 'town', 'cannot', 'having', 'passengers', ':', 'heads', 'does', '?\"', 'supplied', 'room', 'ships', 'mighty', 'country', 'nigh', 'thousand', 'down', 'thou', 'head', 'soul', 'itself', 'it', 'ribs', 'well', 'enough', 'rather', 'was', 'whether', '...', 'pains', 'to', '),', 'were', 'bed', 'spouter', 'what', 'penny', 'aloft', 'a', 'ay', 'men', 'persons', 'told', 'is', 'its', 'matter', 'e', 'maketh', 'stream', 'have', 'pale', 's', 'hear', 'come', 'created', 'going', 'or', 'you', 'because', 'before', 'us', 'sub', 'many', 'deep', 'ye', 'four', 'window', 'sleep', 'beneath', 'around', 'world', 'mouth', 'days', 'american', 'at', 'wild', 'portentous', 'time', 'say', 'see', 'bones', 'air', 'look', 'over', 'death', 'shall', ')', 'don', 'almost', 'fixed', 'which', ',--', 'doubtless', 'else', 'marvellous', 'grand', 'whaling', '.--', 'far', 'any', 'sperm', 'new', 'miles', 'into', 'lord', 'money', 'fishes', 'first', 'seas', 'no', 'full', 'some', 'idea', 'harpoons', '(', 'every', 'true', 'life', 'image', 'jolly', 'green', 'point', 'supper', 'earth', 'works', 'more', 'of', 'can', 'stood', 'has', 'night', 'one', 'tell', 'better', 'still', 'dives', 'floating', 'purse', 'glass', 'either', 'for', 'harpooneer', 'saw', 'looked', 'cook', 'all', 'seen', 'forty', 'fifty', 'high', 'now', 'as', 'sea', 'sort', 'land', 'little', 'might', 'are', 'be', 'out', 'craft', 'let', 'bedford', 'body', ';--', \"'\", 'webster', 'animal', 'nothing', ';', 'heart', 'thing', 'view', 'pacific', 'this', 'kind', 'mast', 'so', 'requires', 'fish', 'whose', 'made', 'must', 'picture', 'yourself', 'few', 'sir', 'will', 'boats', 'hands', 'do', 'low', 'purpose', 'round', 'these', 'half', 'an', 'i', 'jonah', 'two', 'perhaps', '-', 'sailor', 'brought', 'commodore', 'like', 'north', 'destroyed', 'sword', 'get', 'himself', '!', 'stone', 'themselves', 'order', 'extracts', 'off', 'long', 'ten', 'yet', 'called', 'armed', 'also', 'set', '\"', 'seemed', 'those', 'about', 'street', 'door', 'him', 'besides', 'further', 'ever', 'lazarus', 'they', 'the', '.', 'monstrous', 'broiled', 'boat', 'after', 'make', 'once', 'own', 'lay', 'hill', 'had', 'battle', 'stranded', 'house', 'moving', 'narrative', 'but', 'gathered', 'never', 'very', 'stand', 'man', 'coffin', 'others', 'why', 'been', 'letter', 'oil', 'years', 'ishmael', 'king', 'vessel', 'large', 'streets', ',', 'up', 'strong', 'late', 'other', 'euroclydon', 'put', 'wide', 'whenever', 'his', 'thought', 'ship', 'artist', 'them', 'away', 'with', 'however', 'in', 'where', 'between', 'most', 'being', 'leviathan', 'shore', 'length', 'how', 'glasses', 'my', 'wind', 'through', 'behind', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "# remove sparse words\n",
    "vocab = list(set(flatten(corpus)) - set(exclude))\n",
    "vocab.append('<UNK>')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "971a730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word2index dictionary\n",
    "# first key is '<UNK>'\n",
    "\n",
    "word2index = {'<UNK': 0}\n",
    "\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "\n",
    "# create the opposite dictionary index2word\n",
    "\n",
    "index2word = {v: k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80f6e7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10277\n"
     ]
    }
   ],
   "source": [
    "# create the windows of word data to feed into the model\n",
    "# window size mean the number of word before and after the center word\n",
    "WINDOW_SIZE =5\n",
    "\n",
    "# The dummy variable'<DUMMY>' is use to pad \n",
    "# ie if center word is the first word in the sentance need 5 dummy variable in front\n",
    "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE,\n",
    "                                    WINDOW_SIZE * 2 +1)) for c in corpus])\n",
    "print(len(windows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "157451ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville')\n",
      "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851')\n",
      "('<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']')\n",
      "('<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', '<DUMMY>')\n",
      "('<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', '<DUMMY>', '<DUMMY>')\n",
      "('[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', '<DUMMY>', '<DUMMY>', '<DUMMY>')\n",
      "('moby', 'dick', 'by', 'herman', 'melville', '1851', ']', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>')\n",
      "('dick', 'by', 'herman', 'melville', '1851', ']', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>')\n",
      "\n",
      "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', 'etymology', '.', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>')\n"
     ]
    }
   ],
   "source": [
    "# print preview first few windows\n",
    "print(windows[0])\n",
    "print(windows[1])\n",
    "print(windows[2])\n",
    "print(windows[3])\n",
    "print(windows[4])\n",
    "print(windows[5])\n",
    "print(windows[6])\n",
    "print(windows[7])\n",
    "print()\n",
    "print(windows[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a1e11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training set\n",
    "## run through the window in wondows\n",
    "## find the all the context and target pairs\n",
    "## words in exclude list and dummy variable will not be include\n",
    "\n",
    "# window[WINDOW_SIZE]: target word\n",
    "# window[i]          : context word\n",
    "\n",
    "train_data = []\n",
    "for window in windows:\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in exclude or window[WINDOW_SIZE] in exclude:\n",
    "            continue\n",
    "        if window[i] == '<DUMMY>' or i == WINDOW_SIZE:\n",
    "            continue\n",
    "        \n",
    "        train_data.append((window[WINDOW_SIZE], window[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52aebf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(', 'supplied'),\n",
       " ('(', 'by'),\n",
       " ('(', 'a'),\n",
       " ('(', 'late'),\n",
       " ('supplied', '('),\n",
       " ('supplied', 'by'),\n",
       " ('supplied', 'a'),\n",
       " ('supplied', 'late'),\n",
       " ('by', '('),\n",
       " ('by', 'supplied'),\n",
       " ('by', 'a'),\n",
       " ('by', 'late'),\n",
       " ('by', 'to'),\n",
       " ('a', '('),\n",
       " ('a', 'supplied'),\n",
       " ('a', 'by'),\n",
       " ('a', 'late'),\n",
       " ('a', 'to'),\n",
       " ('a', 'a'),\n",
       " ('late', '(')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first few example\n",
    "\n",
    "train_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26734a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train_data into index as postive data\n",
    "\n",
    "\n",
    "X_p = []\n",
    "y_p = []\n",
    "\n",
    "for tr in train_data:\n",
    "    X_p.append(prepare_word(tr[0], word2index).view(1,-1))\n",
    "    y_p.append(prepare_word(tr[1], word2index).view(1,-1))\n",
    "    \n",
    "train_data = list(zip(X_p, y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79a83c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[291]]), tensor([[186]])),\n",
       " (tensor([[291]]), tensor([[130]])),\n",
       " (tensor([[291]]), tensor([[215]])),\n",
       " (tensor([[291]]), tensor([[451]])),\n",
       " (tensor([[186]]), tensor([[291]])),\n",
       " (tensor([[186]]), tensor([[130]])),\n",
       " (tensor([[186]]), tensor([[215]])),\n",
       " (tensor([[186]]), tensor([[451]])),\n",
       " (tensor([[130]]), tensor([[291]])),\n",
       " (tensor([[130]]), tensor([[186]])),\n",
       " (tensor([[130]]), tensor([[215]])),\n",
       " (tensor([[130]]), tensor([[451]])),\n",
       " (tensor([[130]]), tensor([[207]])),\n",
       " (tensor([[215]]), tensor([[291]])),\n",
       " (tensor([[215]]), tensor([[186]])),\n",
       " (tensor([[215]]), tensor([[130]])),\n",
       " (tensor([[215]]), tensor([[451]])),\n",
       " (tensor([[215]]), tensor([[207]])),\n",
       " (tensor([[215]]), tensor([[215]])),\n",
       " (tensor([[451]]), tensor([[291]]))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first few example\n",
    "\n",
    "train_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f407211e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50242"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400be6bf",
   "metadata": {},
   "source": [
    "## 4. Unigram Distribution"
   ]
  },
  {
   "attachments": {
    "Screenshot%202021-12-01%20182858.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3b542c94",
   "metadata": {},
   "source": [
    "Here, we need to create a distribution for negative sampling. Because in a corpus, some words appear more than other words. Hence it will not be fair randomly pick word from that distribution. we need tp create Unigram Distribution.\n",
    "![Screenshot%202021-12-01%20182858.png](attachment:Screenshot%202021-12-01%20182858.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c90459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count= Counter(flatten(corpus))\n",
    "num_total_words = sum([c for w, c in word_count.items() if w not in exclude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "471450de",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 3/4\n",
    "noise_dist = {key: val/num_total_words ** alpha for key, val in word_count.items() if key not in exclude}\n",
    "Z = sum(noise_dist.values())\n",
    "\n",
    "noise_dist_normalized = {key: val/Z for key, val in noise_dist.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74ceb850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization factor Z:        9.397142\n",
      "Noise distribution:            0.0566383\n",
      "Normalized noise distribution: 0.00602719\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalization factor Z:        {:.7}\".format(Z))\n",
    "\n",
    "# noise distribution of the word 'by'\n",
    "print('Noise distribution:            {:.6}'.format(noise_dist[\"by\"]))\n",
    "print('Normalized noise distribution: {:.6}'.format(noise_dist_normalized[\"by\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "572e6767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', ';', '-', 'i', 'well', 'king', ',', 's', 'of', 'such'],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly select 10 words\n",
    "K=10\n",
    "\n",
    "np.random.choice(list(noise_dist_normalized.keys()), size=K, p=list(noise_dist_normalized.values()), replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "166e71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sampling function\n",
    "\n",
    "def negative_sampling(targets, noise_dist_normalized, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        if device =='cuda':\n",
    "            #GPU to CPU\n",
    "            target_index = targets[i].data.cpu().tolist()[0] # PyTorch tensor to list\n",
    "        else:\n",
    "            target_index = targets[i].tolist()[0]\n",
    "    \n",
    "        while len(nsample) < k: #num of sampling\n",
    "            neg = np.random.choice(list(noise_dist_normalized.keys()), size=1, p=list(noise_dist_normalized.values()))\n",
    "            \n",
    "            neg_word = neg[0]\n",
    "            if word2index[neg_word] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg_word)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1,-1))\n",
    "        \n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1f8c40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNegSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SkipgramNegSampling, self).__init__()  \n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_dim) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_dim) # out embedding\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "\n",
    "        nn.init.xavier_normal_(self.embedding_v.weight)\n",
    "        nn.init.xavier_normal_(self.embedding_u.weight)\n",
    "        \n",
    "     \n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "        center_embeds = self.embedding_v(center_words) # B x 1 x D\n",
    "        target_embeds = self.embedding_u(target_words) # B x 1 x D\n",
    "        \n",
    "        neg_embeds = -self.embedding_u(negative_words) # B x K x D\n",
    "\n",
    "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # B x 1\n",
    "        negative_score = torch.sum(neg_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2), 1).view(negs.size(0), -1) # B x K -> B x 1\n",
    "        \n",
    "        loss = self.logsigmoid(positive_score) + self.logsigmoid(negative_score)\n",
    "\n",
    "        return -torch.mean(loss)\n",
    "\n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19e4f0",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1408e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 30 \n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 30\n",
    "NEG = 10 # Num of Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d0ef77df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "loss = []\n",
    "model = SkipgramNegSampling(len(word2index), EMBEDDING_SIZE)\n",
    "if device == 'cuda':\n",
    "    model = model.to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7d30f7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipgramNegSampling(\n",
       "  (embedding_v): Embedding(480, 30)\n",
       "  (embedding_u): Embedding(480, 30)\n",
       "  (logsigmoid): LogSigmoid()\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show model\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2546cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e33da92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 || Iter: 0 || Loss : 1.39 || LR: 0.001000\n",
      "Epoch : 0 || Iter: 20 || Loss : 1.37 || LR: 0.001000\n",
      "Epoch : 0 || Iter: 40 || Loss : 1.34 || LR: 0.001000\n",
      "Epoch : 0 || Iter: 60 || Loss : 1.25 || LR: 0.001000\n",
      "Epoch : 0 || Iter: 80 || Loss : 1.12 || LR: 0.001000\n",
      "Epoch : 0 || Iter: 100 || Loss : 1.04 || LR: 0.001000\n",
      "Epoch : 0 || Iter: 120 || Loss : 0.98 || LR: 0.001000\n",
      "Epoch : 0 || Iter: 140 || Loss : 0.94 || LR: 0.001000\n",
      "Epoch : 0 || Iter: 160 || Loss : 0.93 || LR: 0.001000\n",
      "Epoch : 0 || Iter: 180 || Loss : 0.92 || LR: 0.001000\n",
      "Epoch : 1 || Iter: 0 || Loss : 0.91 || LR: 0.001000\n",
      "Epoch : 1 || Iter: 20 || Loss : 0.91 || LR: 0.001000\n",
      "Epoch : 1 || Iter: 40 || Loss : 0.91 || LR: 0.001000\n",
      "Epoch : 1 || Iter: 60 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 1 || Iter: 80 || Loss : 0.91 || LR: 0.001000\n",
      "Epoch : 1 || Iter: 100 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 1 || Iter: 120 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 1 || Iter: 140 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 1 || Iter: 160 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 1 || Iter: 180 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 2 || Iter: 0 || Loss : 0.89 || LR: 0.001000\n",
      "Epoch : 2 || Iter: 20 || Loss : 0.89 || LR: 0.001000\n",
      "Epoch : 2 || Iter: 40 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 2 || Iter: 60 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 2 || Iter: 80 || Loss : 0.89 || LR: 0.001000\n",
      "Epoch : 2 || Iter: 100 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 2 || Iter: 120 || Loss : 0.89 || LR: 0.001000\n",
      "Epoch : 2 || Iter: 140 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 2 || Iter: 160 || Loss : 0.89 || LR: 0.001000\n",
      "Epoch : 2 || Iter: 180 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 3 || Iter: 0 || Loss : 0.88 || LR: 0.001000\n",
      "Epoch : 3 || Iter: 20 || Loss : 0.89 || LR: 0.001000\n",
      "Epoch : 3 || Iter: 40 || Loss : 0.89 || LR: 0.001000\n",
      "Epoch : 3 || Iter: 60 || Loss : 0.88 || LR: 0.001000\n",
      "Epoch : 3 || Iter: 80 || Loss : 0.89 || LR: 0.001000\n",
      "Epoch : 3 || Iter: 100 || Loss : 0.89 || LR: 0.001000\n",
      "Epoch : 3 || Iter: 120 || Loss : 0.89 || LR: 0.001000\n",
      "Epoch : 3 || Iter: 140 || Loss : 0.89 || LR: 0.001000\n",
      "Epoch : 3 || Iter: 160 || Loss : 0.88 || LR: 0.001000\n",
      "Epoch : 3 || Iter: 180 || Loss : 0.90 || LR: 0.001000\n",
      "Epoch : 4 || Iter: 0 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 4 || Iter: 20 || Loss : 0.89 || LR: 0.000100\n",
      "Epoch : 4 || Iter: 40 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 4 || Iter: 60 || Loss : 0.87 || LR: 0.000100\n",
      "Epoch : 4 || Iter: 80 || Loss : 0.89 || LR: 0.000100\n",
      "Epoch : 4 || Iter: 100 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 4 || Iter: 120 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 4 || Iter: 140 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 4 || Iter: 160 || Loss : 0.89 || LR: 0.000100\n",
      "Epoch : 4 || Iter: 180 || Loss : 0.87 || LR: 0.000100\n",
      "Epoch : 5 || Iter: 0 || Loss : 0.87 || LR: 0.000100\n",
      "Epoch : 5 || Iter: 20 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 5 || Iter: 40 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 5 || Iter: 60 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 5 || Iter: 80 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 5 || Iter: 100 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 5 || Iter: 120 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 5 || Iter: 140 || Loss : 0.87 || LR: 0.000100\n",
      "Epoch : 5 || Iter: 160 || Loss : 0.89 || LR: 0.000100\n",
      "Epoch : 5 || Iter: 180 || Loss : 0.89 || LR: 0.000100\n",
      "Epoch : 6 || Iter: 0 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 6 || Iter: 20 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 6 || Iter: 40 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 6 || Iter: 60 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 6 || Iter: 80 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 6 || Iter: 100 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 6 || Iter: 120 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 6 || Iter: 140 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 6 || Iter: 160 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 6 || Iter: 180 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 7 || Iter: 0 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 7 || Iter: 20 || Loss : 0.87 || LR: 0.000100\n",
      "Epoch : 7 || Iter: 40 || Loss : 0.89 || LR: 0.000100\n",
      "Epoch : 7 || Iter: 60 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 7 || Iter: 80 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 7 || Iter: 100 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 7 || Iter: 120 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 7 || Iter: 140 || Loss : 0.89 || LR: 0.000100\n",
      "Epoch : 7 || Iter: 160 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 7 || Iter: 180 || Loss : 0.87 || LR: 0.000100\n",
      "Epoch : 8 || Iter: 0 || Loss : 0.89 || LR: 0.000100\n",
      "Epoch : 8 || Iter: 20 || Loss : 0.87 || LR: 0.000100\n",
      "Epoch : 8 || Iter: 40 || Loss : 0.87 || LR: 0.000100\n",
      "Epoch : 8 || Iter: 60 || Loss : 0.87 || LR: 0.000100\n",
      "Epoch : 8 || Iter: 80 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 8 || Iter: 100 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 8 || Iter: 120 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 8 || Iter: 140 || Loss : 0.87 || LR: 0.000100\n",
      "Epoch : 8 || Iter: 160 || Loss : 0.88 || LR: 0.000100\n",
      "Epoch : 8 || Iter: 180 || Loss : 0.87 || LR: 0.000100\n",
      "Epoch : 9 || Iter: 0 || Loss : 0.89 || LR: 0.000010\n",
      "Epoch : 9 || Iter: 20 || Loss : 0.89 || LR: 0.000010\n",
      "Epoch : 9 || Iter: 40 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 9 || Iter: 60 || Loss : 0.87 || LR: 0.000010\n",
      "Epoch : 9 || Iter: 80 || Loss : 0.89 || LR: 0.000010\n",
      "Epoch : 9 || Iter: 100 || Loss : 0.87 || LR: 0.000010\n",
      "Epoch : 9 || Iter: 120 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 9 || Iter: 140 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 9 || Iter: 160 || Loss : 0.87 || LR: 0.000010\n",
      "Epoch : 9 || Iter: 180 || Loss : 0.86 || LR: 0.000010\n",
      "Epoch : 10 || Iter: 0 || Loss : 0.87 || LR: 0.000010\n",
      "Epoch : 10 || Iter: 20 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 10 || Iter: 40 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 10 || Iter: 60 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 10 || Iter: 80 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 10 || Iter: 100 || Loss : 0.89 || LR: 0.000010\n",
      "Epoch : 10 || Iter: 120 || Loss : 0.87 || LR: 0.000010\n",
      "Epoch : 10 || Iter: 140 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 10 || Iter: 160 || Loss : 0.89 || LR: 0.000010\n",
      "Epoch : 10 || Iter: 180 || Loss : 0.89 || LR: 0.000010\n",
      "Epoch : 11 || Iter: 0 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 11 || Iter: 20 || Loss : 0.87 || LR: 0.000010\n",
      "Epoch : 11 || Iter: 40 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 11 || Iter: 60 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 11 || Iter: 80 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 11 || Iter: 100 || Loss : 0.87 || LR: 0.000010\n",
      "Epoch : 11 || Iter: 120 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 11 || Iter: 140 || Loss : 0.87 || LR: 0.000010\n",
      "Epoch : 11 || Iter: 160 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 11 || Iter: 180 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 12 || Iter: 0 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 12 || Iter: 20 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 12 || Iter: 40 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 12 || Iter: 60 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 12 || Iter: 80 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 12 || Iter: 100 || Loss : 0.89 || LR: 0.000010\n",
      "Epoch : 12 || Iter: 120 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 12 || Iter: 140 || Loss : 0.89 || LR: 0.000010\n",
      "Epoch : 12 || Iter: 160 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 12 || Iter: 180 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 13 || Iter: 0 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 13 || Iter: 20 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 13 || Iter: 40 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 13 || Iter: 60 || Loss : 0.87 || LR: 0.000010\n",
      "Epoch : 13 || Iter: 80 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 13 || Iter: 100 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 13 || Iter: 120 || Loss : 0.89 || LR: 0.000010\n",
      "Epoch : 13 || Iter: 140 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 13 || Iter: 160 || Loss : 0.89 || LR: 0.000010\n",
      "Epoch : 13 || Iter: 180 || Loss : 0.88 || LR: 0.000010\n",
      "Epoch : 14 || Iter: 0 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 14 || Iter: 20 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 14 || Iter: 40 || Loss : 0.87 || LR: 0.000001\n",
      "Epoch : 14 || Iter: 60 || Loss : 0.89 || LR: 0.000001\n",
      "Epoch : 14 || Iter: 80 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 14 || Iter: 100 || Loss : 0.89 || LR: 0.000001\n",
      "Epoch : 14 || Iter: 120 || Loss : 0.89 || LR: 0.000001\n",
      "Epoch : 14 || Iter: 140 || Loss : 0.87 || LR: 0.000001\n",
      "Epoch : 14 || Iter: 160 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 14 || Iter: 180 || Loss : 0.89 || LR: 0.000001\n",
      "Epoch : 15 || Iter: 0 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 15 || Iter: 20 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 15 || Iter: 40 || Loss : 0.89 || LR: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 15 || Iter: 60 || Loss : 0.87 || LR: 0.000001\n",
      "Epoch : 15 || Iter: 80 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 15 || Iter: 100 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 15 || Iter: 120 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 15 || Iter: 140 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 15 || Iter: 160 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 15 || Iter: 180 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 16 || Iter: 0 || Loss : 0.90 || LR: 0.000001\n",
      "Epoch : 16 || Iter: 20 || Loss : 0.87 || LR: 0.000001\n",
      "Epoch : 16 || Iter: 40 || Loss : 0.89 || LR: 0.000001\n",
      "Epoch : 16 || Iter: 60 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 16 || Iter: 80 || Loss : 0.89 || LR: 0.000001\n",
      "Epoch : 16 || Iter: 100 || Loss : 0.87 || LR: 0.000001\n",
      "Epoch : 16 || Iter: 120 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 16 || Iter: 140 || Loss : 0.89 || LR: 0.000001\n",
      "Epoch : 16 || Iter: 160 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 16 || Iter: 180 || Loss : 0.87 || LR: 0.000001\n",
      "Epoch : 17 || Iter: 0 || Loss : 0.87 || LR: 0.000001\n",
      "Epoch : 17 || Iter: 20 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 17 || Iter: 40 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 17 || Iter: 60 || Loss : 0.87 || LR: 0.000001\n",
      "Epoch : 17 || Iter: 80 || Loss : 0.87 || LR: 0.000001\n",
      "Epoch : 17 || Iter: 100 || Loss : 0.86 || LR: 0.000001\n",
      "Epoch : 17 || Iter: 120 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 17 || Iter: 140 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 17 || Iter: 160 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 17 || Iter: 180 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 18 || Iter: 0 || Loss : 0.89 || LR: 0.000001\n",
      "Epoch : 18 || Iter: 20 || Loss : 0.90 || LR: 0.000001\n",
      "Epoch : 18 || Iter: 40 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 18 || Iter: 60 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 18 || Iter: 80 || Loss : 0.87 || LR: 0.000001\n",
      "Epoch : 18 || Iter: 100 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 18 || Iter: 120 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 18 || Iter: 140 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 18 || Iter: 160 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 18 || Iter: 180 || Loss : 0.88 || LR: 0.000001\n",
      "Epoch : 19 || Iter: 0 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 19 || Iter: 20 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 19 || Iter: 40 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 19 || Iter: 60 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 19 || Iter: 80 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 19 || Iter: 100 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 19 || Iter: 120 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 19 || Iter: 140 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 19 || Iter: 160 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 19 || Iter: 180 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 20 || Iter: 0 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 20 || Iter: 20 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 20 || Iter: 40 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 20 || Iter: 60 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 20 || Iter: 80 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 20 || Iter: 100 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 20 || Iter: 120 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 20 || Iter: 140 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 20 || Iter: 160 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 20 || Iter: 180 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 21 || Iter: 0 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 21 || Iter: 20 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 21 || Iter: 40 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 21 || Iter: 60 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 21 || Iter: 80 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 21 || Iter: 100 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 21 || Iter: 120 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 21 || Iter: 140 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 21 || Iter: 160 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 21 || Iter: 180 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 22 || Iter: 0 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 22 || Iter: 20 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 22 || Iter: 40 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 22 || Iter: 60 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 22 || Iter: 80 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 22 || Iter: 100 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 22 || Iter: 120 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 22 || Iter: 140 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 22 || Iter: 160 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 22 || Iter: 180 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 23 || Iter: 0 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 23 || Iter: 20 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 23 || Iter: 40 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 23 || Iter: 60 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 23 || Iter: 80 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 23 || Iter: 100 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 23 || Iter: 120 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 23 || Iter: 140 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 23 || Iter: 160 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 23 || Iter: 180 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 24 || Iter: 0 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 24 || Iter: 20 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 24 || Iter: 40 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 24 || Iter: 60 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 24 || Iter: 80 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 24 || Iter: 100 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 24 || Iter: 120 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 24 || Iter: 140 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 24 || Iter: 160 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 24 || Iter: 180 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 25 || Iter: 0 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 25 || Iter: 20 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 25 || Iter: 40 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 25 || Iter: 60 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 25 || Iter: 80 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 25 || Iter: 100 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 25 || Iter: 120 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 25 || Iter: 140 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 25 || Iter: 160 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 25 || Iter: 180 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 26 || Iter: 0 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 26 || Iter: 20 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 26 || Iter: 40 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 26 || Iter: 60 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 26 || Iter: 80 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 26 || Iter: 100 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 26 || Iter: 120 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 26 || Iter: 140 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 26 || Iter: 160 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 26 || Iter: 180 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 27 || Iter: 0 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 27 || Iter: 20 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 27 || Iter: 40 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 27 || Iter: 60 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 27 || Iter: 80 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 27 || Iter: 100 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 27 || Iter: 120 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 27 || Iter: 140 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 27 || Iter: 160 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 27 || Iter: 180 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 28 || Iter: 0 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 28 || Iter: 20 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 28 || Iter: 40 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 28 || Iter: 60 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 28 || Iter: 80 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 28 || Iter: 100 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 28 || Iter: 120 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 28 || Iter: 140 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 28 || Iter: 160 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 28 || Iter: 180 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 29 || Iter: 0 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 29 || Iter: 20 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 29 || Iter: 40 || Loss : 0.87 || LR: 0.000000\n",
      "Epoch : 29 || Iter: 60 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 29 || Iter: 80 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 29 || Iter: 100 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 29 || Iter: 120 || Loss : 0.89 || LR: 0.000000\n",
      "Epoch : 29 || Iter: 140 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 29 || Iter: 160 || Loss : 0.88 || LR: 0.000000\n",
      "Epoch : 29 || Iter: 180 || Loss : 0.88 || LR: 0.000000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    scheduler.step()\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        inputs, targets =zip(*batch)\n",
    "        \n",
    "        inputs = torch.cat(inputs).to(device) # B x 1\n",
    "        targets = torch.cat(targets).to(device) # B x 1\n",
    "        negs = negative_sampling(targets, noise_dist_normalized, NEG).to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        loss = model(inputs, targets, negs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 20 ==0:\n",
    "            lr = get_lr(optimizer)\n",
    "            print(f\"Epoch : {epoch} || Iter: {i} || Loss : {loss:.2f} || LR: {lr:.6f}\")        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e64fc3",
   "metadata": {},
   "source": [
    "## 6. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "55855391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(target, vocab):\n",
    "    target_V = model.prediction(prepare_word(target, word2index).to(device))\n",
    "\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target: \n",
    "            continue\n",
    "            \n",
    "        vector = model.prediction(prepare_word(list(vocab)[i], word2index).to(device))\n",
    "        \n",
    "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0]\n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "66ec147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['killed', 0.7773110270500183],\n",
       " ['yourself', 0.7749539017677307],\n",
       " ['young', 0.7704371213912964],\n",
       " ['within', 0.7560813426971436],\n",
       " ['thou', 0.7482627630233765],\n",
       " ['large', 0.7398788332939148],\n",
       " ['side', 0.7367793917655945],\n",
       " ['air', 0.7355208992958069],\n",
       " ['whaling', 0.7344357967376709],\n",
       " ['cook', 0.7241607308387756]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = random.choice(list(vocab))\n",
    "print(test)\n",
    "word_similarity(test, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3495c402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['their', 0.8415759205818176],\n",
       " ['doubtless', 0.8235083222389221],\n",
       " ['gone', 0.8141038417816162],\n",
       " ['lord', 0.811817467212677],\n",
       " ['island', 0.8009188771247864],\n",
       " ['else', 0.7869023084640503],\n",
       " ['aloft', 0.7854390144348145],\n",
       " ['door', 0.7827306389808655],\n",
       " ['stream', 0.7791623473167419],\n",
       " ['looked', 0.7790824174880981]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity('time', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0c1a2",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0619feb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
